<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol.lst-kix_udrb8wyo7nva-3.start{counter-reset:lst-ctn-kix_udrb8wyo7nva-3 0}ol.lst-kix_tggegdf7zkk-0.start{counter-reset:lst-ctn-kix_tggegdf7zkk-0 0}.lst-kix_udrb8wyo7nva-3>li{counter-increment:lst-ctn-kix_udrb8wyo7nva-3}.lst-kix_dkgo1ij4r2di-0>li:before{content:"" counter(lst-ctn-kix_dkgo1ij4r2di-0,decimal) ". "}.lst-kix_dkgo1ij4r2di-2>li:before{content:"" counter(lst-ctn-kix_dkgo1ij4r2di-2,lower-roman) ". "}ol.lst-kix_cz9ekckz0po4-3.start{counter-reset:lst-ctn-kix_cz9ekckz0po4-3 0}.lst-kix_dkgo1ij4r2di-0>li{counter-increment:lst-ctn-kix_dkgo1ij4r2di-0}.lst-kix_dkgo1ij4r2di-3>li:before{content:"" counter(lst-ctn-kix_dkgo1ij4r2di-3,decimal) ". "}.lst-kix_22v9rq33ihyw-3>li{counter-increment:lst-ctn-kix_22v9rq33ihyw-3}ol.lst-kix_bncp17qp0hyr-6.start{counter-reset:lst-ctn-kix_bncp17qp0hyr-6 0}.lst-kix_fkfklindn7va-7>li{counter-increment:lst-ctn-kix_fkfklindn7va-7}.lst-kix_tggegdf7zkk-0>li{counter-increment:lst-ctn-kix_tggegdf7zkk-0}.lst-kix_dkgo1ij4r2di-1>li:before{content:"" counter(lst-ctn-kix_dkgo1ij4r2di-1,lower-latin) ". "}.lst-kix_tggegdf7zkk-8>li:before{content:"" counter(lst-ctn-kix_tggegdf7zkk-8,lower-roman) ". "}.lst-kix_dkgo1ij4r2di-8>li:before{content:"" counter(lst-ctn-kix_dkgo1ij4r2di-8,lower-roman) ". "}.lst-kix_dkgo1ij4r2di-7>li:before{content:"" counter(lst-ctn-kix_dkgo1ij4r2di-7,lower-latin) ". "}ol.lst-kix_22v9rq33ihyw-8{list-style-type:none}.lst-kix_dkgo1ij4r2di-4>li:before{content:"" counter(lst-ctn-kix_dkgo1ij4r2di-4,lower-latin) ". "}.lst-kix_dkgo1ij4r2di-6>li:before{content:"" counter(lst-ctn-kix_dkgo1ij4r2di-6,decimal) ". "}ol.lst-kix_9ux6mvp87acj-2.start{counter-reset:lst-ctn-kix_9ux6mvp87acj-2 0}.lst-kix_dkgo1ij4r2di-5>li:before{content:"" counter(lst-ctn-kix_dkgo1ij4r2di-5,lower-roman) ". "}.lst-kix_9ux6mvp87acj-4>li{counter-increment:lst-ctn-kix_9ux6mvp87acj-4}ol.lst-kix_tggegdf7zkk-6.start{counter-reset:lst-ctn-kix_tggegdf7zkk-6 0}.lst-kix_fkfklindn7va-5>li{counter-increment:lst-ctn-kix_fkfklindn7va-5}.lst-kix_9ux6mvp87acj-2>li{counter-increment:lst-ctn-kix_9ux6mvp87acj-2}ol.lst-kix_tggegdf7zkk-4{list-style-type:none}ol.lst-kix_dkgo1ij4r2di-6.start{counter-reset:lst-ctn-kix_dkgo1ij4r2di-6 0}ol.lst-kix_tggegdf7zkk-5{list-style-type:none}ol.lst-kix_tggegdf7zkk-6{list-style-type:none}.lst-kix_tggegdf7zkk-1>li:before{content:"" counter(lst-ctn-kix_tggegdf7zkk-1,lower-latin) ". "}ol.lst-kix_tggegdf7zkk-7{list-style-type:none}ol.lst-kix_tggegdf7zkk-0{list-style-type:none}ol.lst-kix_tggegdf7zkk-1{list-style-type:none}.lst-kix_dkgo1ij4r2di-2>li{counter-increment:lst-ctn-kix_dkgo1ij4r2di-2}ol.lst-kix_tggegdf7zkk-2{list-style-type:none}.lst-kix_tggegdf7zkk-0>li:before{content:"" counter(lst-ctn-kix_tggegdf7zkk-0,upper-latin) ". "}ol.lst-kix_tggegdf7zkk-3{list-style-type:none}.lst-kix_tggegdf7zkk-5>li:before{content:"" counter(lst-ctn-kix_tggegdf7zkk-5,lower-roman) ". "}.lst-kix_tggegdf7zkk-7>li:before{content:"" counter(lst-ctn-kix_tggegdf7zkk-7,lower-latin) ". "}ol.lst-kix_fkfklindn7va-3.start{counter-reset:lst-ctn-kix_fkfklindn7va-3 0}ol.lst-kix_tggegdf7zkk-8{list-style-type:none}ol.lst-kix_22v9rq33ihyw-5.start{counter-reset:lst-ctn-kix_22v9rq33ihyw-5 0}.lst-kix_tggegdf7zkk-2>li:before{content:"" counter(lst-ctn-kix_tggegdf7zkk-2,lower-roman) ". "}.lst-kix_tggegdf7zkk-6>li:before{content:"" counter(lst-ctn-kix_tggegdf7zkk-6,decimal) ". "}ol.lst-kix_dkgo1ij4r2di-0.start{counter-reset:lst-ctn-kix_dkgo1ij4r2di-0 0}.lst-kix_tggegdf7zkk-3>li:before{content:"" counter(lst-ctn-kix_tggegdf7zkk-3,decimal) ". "}.lst-kix_udrb8wyo7nva-1>li{counter-increment:lst-ctn-kix_udrb8wyo7nva-1}.lst-kix_22v9rq33ihyw-5>li{counter-increment:lst-ctn-kix_22v9rq33ihyw-5}ol.lst-kix_cz9ekckz0po4-8.start{counter-reset:lst-ctn-kix_cz9ekckz0po4-8 0}.lst-kix_tggegdf7zkk-4>li:before{content:"" counter(lst-ctn-kix_tggegdf7zkk-4,lower-latin) ". "}.lst-kix_9ux6mvp87acj-5>li:before{content:"" counter(lst-ctn-kix_9ux6mvp87acj-5,lower-roman) ". "}.lst-kix_9ux6mvp87acj-7>li:before{content:"" counter(lst-ctn-kix_9ux6mvp87acj-7,lower-latin) ". "}.lst-kix_9ux6mvp87acj-3>li:before{content:"" counter(lst-ctn-kix_9ux6mvp87acj-3,decimal) ". "}.lst-kix_9ux6mvp87acj-6>li:before{content:"" counter(lst-ctn-kix_9ux6mvp87acj-6,decimal) ". "}ol.lst-kix_9ux6mvp87acj-7.start{counter-reset:lst-ctn-kix_9ux6mvp87acj-7 0}.lst-kix_9ux6mvp87acj-6>li{counter-increment:lst-ctn-kix_9ux6mvp87acj-6}.lst-kix_9ux6mvp87acj-1>li:before{content:"" counter(lst-ctn-kix_9ux6mvp87acj-1,lower-latin) ". "}.lst-kix_9ux6mvp87acj-2>li:before{content:"" counter(lst-ctn-kix_9ux6mvp87acj-2,lower-roman) ". "}.lst-kix_cz9ekckz0po4-1>li{counter-increment:lst-ctn-kix_cz9ekckz0po4-1}.lst-kix_9ux6mvp87acj-0>li:before{content:"" counter(lst-ctn-kix_9ux6mvp87acj-0,decimal) ". "}.lst-kix_9ux6mvp87acj-0>li{counter-increment:lst-ctn-kix_9ux6mvp87acj-0}.lst-kix_9ux6mvp87acj-8>li:before{content:"" counter(lst-ctn-kix_9ux6mvp87acj-8,lower-roman) ". "}ol.lst-kix_udrb8wyo7nva-8.start{counter-reset:lst-ctn-kix_udrb8wyo7nva-8 0}.lst-kix_bncp17qp0hyr-8>li{counter-increment:lst-ctn-kix_bncp17qp0hyr-8}.lst-kix_bncp17qp0hyr-1>li{counter-increment:lst-ctn-kix_bncp17qp0hyr-1}ol.lst-kix_udrb8wyo7nva-2.start{counter-reset:lst-ctn-kix_udrb8wyo7nva-2 0}.lst-kix_9ux6mvp87acj-4>li:before{content:"" counter(lst-ctn-kix_9ux6mvp87acj-4,lower-latin) ". "}ol.lst-kix_fkfklindn7va-4.start{counter-reset:lst-ctn-kix_fkfklindn7va-4 0}ol.lst-kix_tggegdf7zkk-1.start{counter-reset:lst-ctn-kix_tggegdf7zkk-1 0}.lst-kix_22v9rq33ihyw-7>li{counter-increment:lst-ctn-kix_22v9rq33ihyw-7}ol.lst-kix_dkgo1ij4r2di-1.start{counter-reset:lst-ctn-kix_dkgo1ij4r2di-1 0}.lst-kix_tggegdf7zkk-7>li{counter-increment:lst-ctn-kix_tggegdf7zkk-7}ol.lst-kix_22v9rq33ihyw-7{list-style-type:none}ol.lst-kix_22v9rq33ihyw-6{list-style-type:none}ol.lst-kix_22v9rq33ihyw-5{list-style-type:none}ol.lst-kix_22v9rq33ihyw-4{list-style-type:none}ol.lst-kix_9ux6mvp87acj-8.start{counter-reset:lst-ctn-kix_9ux6mvp87acj-8 0}ol.lst-kix_22v9rq33ihyw-3{list-style-type:none}.lst-kix_22v9rq33ihyw-1>li{counter-increment:lst-ctn-kix_22v9rq33ihyw-1}ol.lst-kix_22v9rq33ihyw-2{list-style-type:none}ol.lst-kix_22v9rq33ihyw-1{list-style-type:none}ol.lst-kix_22v9rq33ihyw-4.start{counter-reset:lst-ctn-kix_22v9rq33ihyw-4 0}ol.lst-kix_22v9rq33ihyw-0{list-style-type:none}ol.lst-kix_9ux6mvp87acj-1.start{counter-reset:lst-ctn-kix_9ux6mvp87acj-1 0}.lst-kix_tggegdf7zkk-4>li{counter-increment:lst-ctn-kix_tggegdf7zkk-4}.lst-kix_bncp17qp0hyr-7>li:before{content:"" counter(lst-ctn-kix_bncp17qp0hyr-7,lower-latin) ". "}.lst-kix_bncp17qp0hyr-5>li:before{content:"" counter(lst-ctn-kix_bncp17qp0hyr-5,lower-roman) ". "}.lst-kix_cz9ekckz0po4-4>li{counter-increment:lst-ctn-kix_cz9ekckz0po4-4}.lst-kix_fkfklindn7va-1>li{counter-increment:lst-ctn-kix_fkfklindn7va-1}.lst-kix_bncp17qp0hyr-5>li{counter-increment:lst-ctn-kix_bncp17qp0hyr-5}.lst-kix_udrb8wyo7nva-0>li:before{content:"" counter(lst-ctn-kix_udrb8wyo7nva-0,decimal) ") "}.lst-kix_bncp17qp0hyr-0>li:before{content:"" counter(lst-ctn-kix_bncp17qp0hyr-0,decimal) ". "}ol.lst-kix_fkfklindn7va-2.start{counter-reset:lst-ctn-kix_fkfklindn7va-2 0}.lst-kix_fkfklindn7va-0>li{counter-increment:lst-ctn-kix_fkfklindn7va-0}ol.lst-kix_bncp17qp0hyr-2.start{counter-reset:lst-ctn-kix_bncp17qp0hyr-2 0}.lst-kix_udrb8wyo7nva-6>li:before{content:"" counter(lst-ctn-kix_udrb8wyo7nva-6,decimal) ". "}ol.lst-kix_cz9ekckz0po4-7.start{counter-reset:lst-ctn-kix_cz9ekckz0po4-7 0}ol.lst-kix_22v9rq33ihyw-3.start{counter-reset:lst-ctn-kix_22v9rq33ihyw-3 0}.lst-kix_udrb8wyo7nva-2>li:before{content:"" counter(lst-ctn-kix_udrb8wyo7nva-2,lower-roman) ") "}.lst-kix_udrb8wyo7nva-4>li:before{content:"(" counter(lst-ctn-kix_udrb8wyo7nva-4,lower-latin) ") "}ol.lst-kix_9ux6mvp87acj-1{list-style-type:none}ol.lst-kix_9ux6mvp87acj-2{list-style-type:none}ol.lst-kix_9ux6mvp87acj-0{list-style-type:none}ol.lst-kix_9ux6mvp87acj-7{list-style-type:none}ol.lst-kix_9ux6mvp87acj-8{list-style-type:none}ol.lst-kix_9ux6mvp87acj-5{list-style-type:none}ol.lst-kix_9ux6mvp87acj-6{list-style-type:none}.lst-kix_udrb8wyo7nva-8>li:before{content:"" counter(lst-ctn-kix_udrb8wyo7nva-8,lower-roman) ". "}ol.lst-kix_9ux6mvp87acj-3{list-style-type:none}.lst-kix_cz9ekckz0po4-6>li{counter-increment:lst-ctn-kix_cz9ekckz0po4-6}ol.lst-kix_9ux6mvp87acj-4{list-style-type:none}ol.lst-kix_9ux6mvp87acj-3.start{counter-reset:lst-ctn-kix_9ux6mvp87acj-3 0}.lst-kix_dkgo1ij4r2di-5>li{counter-increment:lst-ctn-kix_dkgo1ij4r2di-5}.lst-kix_22v9rq33ihyw-8>li{counter-increment:lst-ctn-kix_22v9rq33ihyw-8}ol.lst-kix_fkfklindn7va-0.start{counter-reset:lst-ctn-kix_fkfklindn7va-0 0}ol.lst-kix_9ux6mvp87acj-6.start{counter-reset:lst-ctn-kix_9ux6mvp87acj-6 0}.lst-kix_cz9ekckz0po4-4>li:before{content:"" counter(lst-ctn-kix_cz9ekckz0po4-4,lower-latin) ". "}ol.lst-kix_fkfklindn7va-6{list-style-type:none}ol.lst-kix_fkfklindn7va-7{list-style-type:none}ol.lst-kix_fkfklindn7va-8{list-style-type:none}ol.lst-kix_22v9rq33ihyw-1.start{counter-reset:lst-ctn-kix_22v9rq33ihyw-1 0}.lst-kix_cz9ekckz0po4-0>li:before{content:"" counter(lst-ctn-kix_cz9ekckz0po4-0,upper-latin) ". "}.lst-kix_cz9ekckz0po4-8>li:before{content:"" counter(lst-ctn-kix_cz9ekckz0po4-8,lower-roman) ". "}.lst-kix_tggegdf7zkk-3>li{counter-increment:lst-ctn-kix_tggegdf7zkk-3}.lst-kix_cz9ekckz0po4-6>li:before{content:"" counter(lst-ctn-kix_cz9ekckz0po4-6,decimal) ". "}.lst-kix_tggegdf7zkk-8>li{counter-increment:lst-ctn-kix_tggegdf7zkk-8}.lst-kix_tggegdf7zkk-2>li{counter-increment:lst-ctn-kix_tggegdf7zkk-2}ol.lst-kix_fkfklindn7va-0{list-style-type:none}ol.lst-kix_fkfklindn7va-1{list-style-type:none}ol.lst-kix_fkfklindn7va-2{list-style-type:none}ol.lst-kix_fkfklindn7va-3{list-style-type:none}ol.lst-kix_fkfklindn7va-4{list-style-type:none}ol.lst-kix_fkfklindn7va-5{list-style-type:none}ol.lst-kix_bncp17qp0hyr-0.start{counter-reset:lst-ctn-kix_bncp17qp0hyr-0 0}.lst-kix_bncp17qp0hyr-6>li{counter-increment:lst-ctn-kix_bncp17qp0hyr-6}.lst-kix_dkgo1ij4r2di-4>li{counter-increment:lst-ctn-kix_dkgo1ij4r2di-4}ol.lst-kix_cz9ekckz0po4-2{list-style-type:none}ol.lst-kix_cz9ekckz0po4-3{list-style-type:none}ol.lst-kix_9ux6mvp87acj-4.start{counter-reset:lst-ctn-kix_9ux6mvp87acj-4 0}ol.lst-kix_22v9rq33ihyw-0.start{counter-reset:lst-ctn-kix_22v9rq33ihyw-0 0}ol.lst-kix_cz9ekckz0po4-0{list-style-type:none}ol.lst-kix_cz9ekckz0po4-1{list-style-type:none}ol.lst-kix_cz9ekckz0po4-6{list-style-type:none}.lst-kix_bncp17qp0hyr-0>li{counter-increment:lst-ctn-kix_bncp17qp0hyr-0}ol.lst-kix_cz9ekckz0po4-7{list-style-type:none}ol.lst-kix_cz9ekckz0po4-4{list-style-type:none}ol.lst-kix_dkgo1ij4r2di-0{list-style-type:none}ol.lst-kix_cz9ekckz0po4-5{list-style-type:none}ol.lst-kix_dkgo1ij4r2di-1{list-style-type:none}ol.lst-kix_dkgo1ij4r2di-2{list-style-type:none}ol.lst-kix_dkgo1ij4r2di-3{list-style-type:none}ol.lst-kix_cz9ekckz0po4-8{list-style-type:none}.lst-kix_udrb8wyo7nva-5>li{counter-increment:lst-ctn-kix_udrb8wyo7nva-5}ol.lst-kix_dkgo1ij4r2di-4{list-style-type:none}.lst-kix_cz9ekckz0po4-5>li{counter-increment:lst-ctn-kix_cz9ekckz0po4-5}ol.lst-kix_dkgo1ij4r2di-5{list-style-type:none}ol.lst-kix_bncp17qp0hyr-1.start{counter-reset:lst-ctn-kix_bncp17qp0hyr-1 0}ol.lst-kix_dkgo1ij4r2di-6{list-style-type:none}ol.lst-kix_dkgo1ij4r2di-7{list-style-type:none}ol.lst-kix_dkgo1ij4r2di-8{list-style-type:none}.lst-kix_cz9ekckz0po4-2>li:before{content:"" counter(lst-ctn-kix_cz9ekckz0po4-2,lower-roman) ". "}ol.lst-kix_9ux6mvp87acj-5.start{counter-reset:lst-ctn-kix_9ux6mvp87acj-5 0}.lst-kix_9ux6mvp87acj-3>li{counter-increment:lst-ctn-kix_9ux6mvp87acj-3}ol.lst-kix_tggegdf7zkk-3.start{counter-reset:lst-ctn-kix_tggegdf7zkk-3 0}ol.lst-kix_udrb8wyo7nva-0.start{counter-reset:lst-ctn-kix_udrb8wyo7nva-0 0}ol.lst-kix_fkfklindn7va-1.start{counter-reset:lst-ctn-kix_fkfklindn7va-1 0}.lst-kix_fkfklindn7va-6>li{counter-increment:lst-ctn-kix_fkfklindn7va-6}.lst-kix_22v9rq33ihyw-2>li:before{content:"" counter(lst-ctn-kix_22v9rq33ihyw-2,lower-roman) ". "}.lst-kix_22v9rq33ihyw-3>li:before{content:"" counter(lst-ctn-kix_22v9rq33ihyw-3,decimal) ". "}.lst-kix_9ux6mvp87acj-5>li{counter-increment:lst-ctn-kix_9ux6mvp87acj-5}.lst-kix_cz9ekckz0po4-0>li{counter-increment:lst-ctn-kix_cz9ekckz0po4-0}.lst-kix_22v9rq33ihyw-0>li:before{content:"" counter(lst-ctn-kix_22v9rq33ihyw-0,decimal) ". "}.lst-kix_22v9rq33ihyw-4>li:before{content:"" counter(lst-ctn-kix_22v9rq33ihyw-4,lower-latin) ". "}.lst-kix_udrb8wyo7nva-4>li{counter-increment:lst-ctn-kix_udrb8wyo7nva-4}.lst-kix_fkfklindn7va-7>li:before{content:"" counter(lst-ctn-kix_fkfklindn7va-7,lower-latin) ". "}.lst-kix_22v9rq33ihyw-6>li:before{content:"" counter(lst-ctn-kix_22v9rq33ihyw-6,decimal) ". "}.lst-kix_22v9rq33ihyw-7>li:before{content:"" counter(lst-ctn-kix_22v9rq33ihyw-7,lower-latin) ". "}ol.lst-kix_bncp17qp0hyr-3.start{counter-reset:lst-ctn-kix_bncp17qp0hyr-3 0}ol.lst-kix_dkgo1ij4r2di-3.start{counter-reset:lst-ctn-kix_dkgo1ij4r2di-3 0}ol.lst-kix_fkfklindn7va-6.start{counter-reset:lst-ctn-kix_fkfklindn7va-6 0}.lst-kix_fkfklindn7va-8>li:before{content:"" counter(lst-ctn-kix_fkfklindn7va-8,lower-roman) ". "}.lst-kix_22v9rq33ihyw-5>li:before{content:"" counter(lst-ctn-kix_22v9rq33ihyw-5,lower-roman) ". "}.lst-kix_22v9rq33ihyw-2>li{counter-increment:lst-ctn-kix_22v9rq33ihyw-2}ol.lst-kix_udrb8wyo7nva-6.start{counter-reset:lst-ctn-kix_udrb8wyo7nva-6 0}.lst-kix_22v9rq33ihyw-1>li:before{content:"" counter(lst-ctn-kix_22v9rq33ihyw-1,lower-latin) ". "}.lst-kix_fkfklindn7va-8>li{counter-increment:lst-ctn-kix_fkfklindn7va-8}ol.lst-kix_22v9rq33ihyw-2.start{counter-reset:lst-ctn-kix_22v9rq33ihyw-2 0}ol.lst-kix_dkgo1ij4r2di-2.start{counter-reset:lst-ctn-kix_dkgo1ij4r2di-2 0}ol.lst-kix_9ux6mvp87acj-0.start{counter-reset:lst-ctn-kix_9ux6mvp87acj-0 0}.lst-kix_cz9ekckz0po4-7>li{counter-increment:lst-ctn-kix_cz9ekckz0po4-7}.lst-kix_dkgo1ij4r2di-3>li{counter-increment:lst-ctn-kix_dkgo1ij4r2di-3}ol.lst-kix_cz9ekckz0po4-5.start{counter-reset:lst-ctn-kix_cz9ekckz0po4-5 0}.lst-kix_udrb8wyo7nva-0>li{counter-increment:lst-ctn-kix_udrb8wyo7nva-0}.lst-kix_fkfklindn7va-0>li:before{content:"" counter(lst-ctn-kix_fkfklindn7va-0,decimal) ". "}.lst-kix_fkfklindn7va-1>li:before{content:"" counter(lst-ctn-kix_fkfklindn7va-1,lower-latin) ". "}.lst-kix_fkfklindn7va-4>li{counter-increment:lst-ctn-kix_fkfklindn7va-4}.lst-kix_fkfklindn7va-2>li:before{content:"" counter(lst-ctn-kix_fkfklindn7va-2,lower-roman) ". "}.lst-kix_fkfklindn7va-3>li:before{content:"" counter(lst-ctn-kix_fkfklindn7va-3,decimal) ". "}ol.lst-kix_cz9ekckz0po4-1.start{counter-reset:lst-ctn-kix_cz9ekckz0po4-1 0}.lst-kix_fkfklindn7va-6>li:before{content:"" counter(lst-ctn-kix_fkfklindn7va-6,decimal) ". "}.lst-kix_bncp17qp0hyr-7>li{counter-increment:lst-ctn-kix_bncp17qp0hyr-7}.lst-kix_udrb8wyo7nva-6>li{counter-increment:lst-ctn-kix_udrb8wyo7nva-6}.lst-kix_fkfklindn7va-4>li:before{content:"" counter(lst-ctn-kix_fkfklindn7va-4,lower-latin) ". "}.lst-kix_fkfklindn7va-5>li:before{content:"" counter(lst-ctn-kix_fkfklindn7va-5,lower-roman) ". "}ol.lst-kix_bncp17qp0hyr-7{list-style-type:none}ol.lst-kix_bncp17qp0hyr-6{list-style-type:none}ol.lst-kix_bncp17qp0hyr-5{list-style-type:none}ol.lst-kix_bncp17qp0hyr-4{list-style-type:none}ol.lst-kix_bncp17qp0hyr-3{list-style-type:none}ol.lst-kix_bncp17qp0hyr-2{list-style-type:none}.lst-kix_22v9rq33ihyw-4>li{counter-increment:lst-ctn-kix_22v9rq33ihyw-4}ol.lst-kix_bncp17qp0hyr-1{list-style-type:none}ol.lst-kix_dkgo1ij4r2di-8.start{counter-reset:lst-ctn-kix_dkgo1ij4r2di-8 0}ol.lst-kix_bncp17qp0hyr-0{list-style-type:none}ol.lst-kix_tggegdf7zkk-8.start{counter-reset:lst-ctn-kix_tggegdf7zkk-8 0}ol.lst-kix_cz9ekckz0po4-6.start{counter-reset:lst-ctn-kix_cz9ekckz0po4-6 0}ol.lst-kix_cz9ekckz0po4-0.start{counter-reset:lst-ctn-kix_cz9ekckz0po4-0 0}.lst-kix_bncp17qp0hyr-1>li:before{content:"" counter(lst-ctn-kix_bncp17qp0hyr-1,lower-latin) ". "}.lst-kix_bncp17qp0hyr-2>li:before{content:"" counter(lst-ctn-kix_bncp17qp0hyr-2,lower-roman) ". "}ol.lst-kix_tggegdf7zkk-2.start{counter-reset:lst-ctn-kix_tggegdf7zkk-2 0}.lst-kix_bncp17qp0hyr-3>li:before{content:"" counter(lst-ctn-kix_bncp17qp0hyr-3,decimal) ". "}ol.lst-kix_bncp17qp0hyr-4.start{counter-reset:lst-ctn-kix_bncp17qp0hyr-4 0}.lst-kix_tggegdf7zkk-1>li{counter-increment:lst-ctn-kix_tggegdf7zkk-1}ol.lst-kix_udrb8wyo7nva-1.start{counter-reset:lst-ctn-kix_udrb8wyo7nva-1 0}.lst-kix_bncp17qp0hyr-4>li:before{content:"" counter(lst-ctn-kix_bncp17qp0hyr-4,lower-latin) ". "}ol.lst-kix_bncp17qp0hyr-8{list-style-type:none}ol.lst-kix_fkfklindn7va-5.start{counter-reset:lst-ctn-kix_fkfklindn7va-5 0}.lst-kix_bncp17qp0hyr-6>li:before{content:"" counter(lst-ctn-kix_bncp17qp0hyr-6,decimal) ". "}.lst-kix_udrb8wyo7nva-1>li:before{content:"" counter(lst-ctn-kix_udrb8wyo7nva-1,lower-latin) ") "}ol.lst-kix_tggegdf7zkk-7.start{counter-reset:lst-ctn-kix_tggegdf7zkk-7 0}ol.lst-kix_dkgo1ij4r2di-7.start{counter-reset:lst-ctn-kix_dkgo1ij4r2di-7 0}.lst-kix_bncp17qp0hyr-8>li:before{content:"" counter(lst-ctn-kix_bncp17qp0hyr-8,lower-roman) ". "}.lst-kix_tggegdf7zkk-5>li{counter-increment:lst-ctn-kix_tggegdf7zkk-5}.lst-kix_udrb8wyo7nva-5>li:before{content:"(" counter(lst-ctn-kix_udrb8wyo7nva-5,lower-roman) ") "}ol.lst-kix_dkgo1ij4r2di-4.start{counter-reset:lst-ctn-kix_dkgo1ij4r2di-4 0}ol.lst-kix_udrb8wyo7nva-7.start{counter-reset:lst-ctn-kix_udrb8wyo7nva-7 0}ol.lst-kix_tggegdf7zkk-4.start{counter-reset:lst-ctn-kix_tggegdf7zkk-4 0}.lst-kix_tggegdf7zkk-6>li{counter-increment:lst-ctn-kix_tggegdf7zkk-6}ol.lst-kix_22v9rq33ihyw-6.start{counter-reset:lst-ctn-kix_22v9rq33ihyw-6 0}.lst-kix_bncp17qp0hyr-4>li{counter-increment:lst-ctn-kix_bncp17qp0hyr-4}.lst-kix_udrb8wyo7nva-3>li:before{content:"(" counter(lst-ctn-kix_udrb8wyo7nva-3,decimal) ") "}.lst-kix_dkgo1ij4r2di-6>li{counter-increment:lst-ctn-kix_dkgo1ij4r2di-6}ol.lst-kix_udrb8wyo7nva-5{list-style-type:none}ol.lst-kix_cz9ekckz0po4-4.start{counter-reset:lst-ctn-kix_cz9ekckz0po4-4 0}ol.lst-kix_udrb8wyo7nva-6{list-style-type:none}ol.lst-kix_udrb8wyo7nva-7{list-style-type:none}ol.lst-kix_bncp17qp0hyr-5.start{counter-reset:lst-ctn-kix_bncp17qp0hyr-5 0}ol.lst-kix_udrb8wyo7nva-8{list-style-type:none}.lst-kix_fkfklindn7va-2>li{counter-increment:lst-ctn-kix_fkfklindn7va-2}ol.lst-kix_udrb8wyo7nva-1{list-style-type:none}ol.lst-kix_udrb8wyo7nva-2{list-style-type:none}ol.lst-kix_udrb8wyo7nva-3{list-style-type:none}ol.lst-kix_udrb8wyo7nva-4{list-style-type:none}ol.lst-kix_cz9ekckz0po4-2.start{counter-reset:lst-ctn-kix_cz9ekckz0po4-2 0}.lst-kix_udrb8wyo7nva-7>li{counter-increment:lst-ctn-kix_udrb8wyo7nva-7}.lst-kix_cz9ekckz0po4-3>li{counter-increment:lst-ctn-kix_cz9ekckz0po4-3}.lst-kix_udrb8wyo7nva-7>li:before{content:"" counter(lst-ctn-kix_udrb8wyo7nva-7,lower-latin) ". "}.lst-kix_9ux6mvp87acj-8>li{counter-increment:lst-ctn-kix_9ux6mvp87acj-8}.lst-kix_dkgo1ij4r2di-8>li{counter-increment:lst-ctn-kix_dkgo1ij4r2di-8}.lst-kix_bncp17qp0hyr-2>li{counter-increment:lst-ctn-kix_bncp17qp0hyr-2}ol.lst-kix_udrb8wyo7nva-0{list-style-type:none}.lst-kix_cz9ekckz0po4-3>li:before{content:"" counter(lst-ctn-kix_cz9ekckz0po4-3,decimal) ". "}.lst-kix_cz9ekckz0po4-5>li:before{content:"" counter(lst-ctn-kix_cz9ekckz0po4-5,lower-roman) ". "}ol.lst-kix_22v9rq33ihyw-8.start{counter-reset:lst-ctn-kix_22v9rq33ihyw-8 0}.lst-kix_22v9rq33ihyw-0>li{counter-increment:lst-ctn-kix_22v9rq33ihyw-0}.lst-kix_22v9rq33ihyw-6>li{counter-increment:lst-ctn-kix_22v9rq33ihyw-6}.lst-kix_cz9ekckz0po4-7>li:before{content:"" counter(lst-ctn-kix_cz9ekckz0po4-7,lower-latin) ". "}ol.lst-kix_fkfklindn7va-7.start{counter-reset:lst-ctn-kix_fkfklindn7va-7 0}ol.lst-kix_bncp17qp0hyr-8.start{counter-reset:lst-ctn-kix_bncp17qp0hyr-8 0}.lst-kix_fkfklindn7va-3>li{counter-increment:lst-ctn-kix_fkfklindn7va-3}.lst-kix_9ux6mvp87acj-7>li{counter-increment:lst-ctn-kix_9ux6mvp87acj-7}.lst-kix_9ux6mvp87acj-1>li{counter-increment:lst-ctn-kix_9ux6mvp87acj-1}.lst-kix_22v9rq33ihyw-8>li:before{content:"" counter(lst-ctn-kix_22v9rq33ihyw-8,lower-roman) ". "}ol.lst-kix_udrb8wyo7nva-5.start{counter-reset:lst-ctn-kix_udrb8wyo7nva-5 0}.lst-kix_bncp17qp0hyr-3>li{counter-increment:lst-ctn-kix_bncp17qp0hyr-3}ol.lst-kix_22v9rq33ihyw-7.start{counter-reset:lst-ctn-kix_22v9rq33ihyw-7 0}.lst-kix_dkgo1ij4r2di-7>li{counter-increment:lst-ctn-kix_dkgo1ij4r2di-7}ol.lst-kix_udrb8wyo7nva-4.start{counter-reset:lst-ctn-kix_udrb8wyo7nva-4 0}ol.lst-kix_bncp17qp0hyr-7.start{counter-reset:lst-ctn-kix_bncp17qp0hyr-7 0}.lst-kix_udrb8wyo7nva-2>li{counter-increment:lst-ctn-kix_udrb8wyo7nva-2}.lst-kix_cz9ekckz0po4-2>li{counter-increment:lst-ctn-kix_cz9ekckz0po4-2}.lst-kix_udrb8wyo7nva-8>li{counter-increment:lst-ctn-kix_udrb8wyo7nva-8}ol.lst-kix_dkgo1ij4r2di-5.start{counter-reset:lst-ctn-kix_dkgo1ij4r2di-5 0}ol.lst-kix_fkfklindn7va-8.start{counter-reset:lst-ctn-kix_fkfklindn7va-8 0}.lst-kix_cz9ekckz0po4-8>li{counter-increment:lst-ctn-kix_cz9ekckz0po4-8}.lst-kix_dkgo1ij4r2di-1>li{counter-increment:lst-ctn-kix_dkgo1ij4r2di-1}.lst-kix_cz9ekckz0po4-1>li:before{content:"" counter(lst-ctn-kix_cz9ekckz0po4-1,lower-latin) ". "}ol.lst-kix_tggegdf7zkk-5.start{counter-reset:lst-ctn-kix_tggegdf7zkk-5 0}ol{margin:0;padding:0}table td,table th{padding:0}.c33{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left;height:14pt}.c43{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c48{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c28{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c50{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c11{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c2{margin-left:36pt;padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c15{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Arial";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c1{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c51{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c49{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:center}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c38{color:#000000;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c39{color:#e06666;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c26{padding-top:18pt;padding-bottom:4pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c35{color:#000000;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c34{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c46{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial"}.c45{color:#000000;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c20{color:#434343;text-decoration:none;vertical-align:baseline;font-family:"Arial";font-style:normal}.c63{color:#000000;text-decoration:none;vertical-align:baseline;font-family:"Arial"}.c17{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c47{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;text-decoration:underline}.c56{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c5{background-color:#d9ead3;font-style:italic}.c7{color:inherit;text-decoration:inherit}.c25{margin-left:72pt;padding-left:0pt}.c12{font-size:36pt;font-style:italic}.c10{font-size:9pt;font-style:italic}.c23{padding:0;margin:0}.c42{font-size:10pt}.c24{background-color:#93c47d}.c31{background-color:#ea9999}.c65{background-color:#00ff00}.c69{margin-left:36pt}.c40{background-color:#fff2cc}.c58{font-size:9pt}.c41{background-color:#f1c232}.c3{font-weight:700}.c61{background-color:#dd7e6b}.c36{font-size:14pt}.c57{font-size:18pt}.c64{background-color:#ff00ff}.c37{font-weight:400}.c27{font-style:italic}.c44{background-color:#cfe2f3}.c22{background-color:#e69138}.c60{background-color:#f9cb9c}.c62{background-color:#b6d7a8}.c54{background-color:#a2c4c9}.c18{background-color:#f4cccc}.c19{background-color:#6aa84f}.c67{height:20pt}.c30{background-color:#4a86e8}.c8{height:11pt}.c71{font-size:16pt}.c9{background-color:#ff9900}.c53{background-color:#d9ead3}.c70{background-color:#f6b26b}.c32{background-color:#ead1dc}.c14{background-color:#ffe599}.c68{font-size:30pt}.c52{font-size:8pt}.c55{padding-left:0pt}.c59{height:16pt}.c21{background-color:#e6b8af}.c29{background-color:#fce5cd}.c66{font-size:17pt}.c16{font-size:24pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c56"><p class="c4"><span class="c42 c3">Disclaimer</span><span class="c42">&nbsp;: these are early notes for understanding attention based models, i could be completely wrong in understanding any of the concepts, therefore, after you take a look at this please, </span><span class="c37 c21 c38">rely only on the source papers to properly understand this subject. </span></p><p class="c4 c8"><span class="c6 c21"></span></p><h1 class="c51" id="h.dv26r4mvc2s0"><span class="c45 c3 c68">SEQUENCE MODELS</span></h1><p class="c4"><span class="c6">Sequence models are capable of working with sequential data, ie data that moves through time</span></p><p class="c4"><span class="c6">Examples : </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c3">Video</span><span class="c6">&nbsp;(sequence of image frames)</span></p><p class="c4"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c3">Audio</span><span class="c6">( audio signal), (live recording from a mic)</span></p><p class="c4"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c3">Text</span><span class="c6">&nbsp;(sequence of letters, characters)</span></p><p class="c4"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span><span class="c3">Sensor data</span><span class="c6">, or any kind of data stream</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 485.50px; height: 325.88px;"><img alt="" src="images/image1.jpg" style="width: 485.50px; height: 1934.52px; margin-left: 0.00px; margin-top: -42.51px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c0"><span class="c34 c27">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;It is possible also to have sequence data only as input/output and the other side being a vector. &nbsp;Seq to vec, &nbsp;seq to seq, &nbsp;vec to seq.</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6">Sequence to sequence deep learning models use cases : </span></p><ol class="c23 lst-kix_22v9rq33ihyw-0 start" start="1"><li class="c4 c25"><span class="c6">Machine translation</span></li><li class="c4 c25"><span class="c6">Text summarization</span></li><li class="c4 c25"><span class="c6">Image captioning </span></li><li class="c4 c25"><span class="c6">video/audio generation or manipulation, </span></li><li class="c4 c25"><span class="c6">Text to speech and vice versa. </span></li><li class="c4 c25"><span class="c6">Video super resolution. </span></li></ol><p class="c4"><span>two pioneering papers (</span><span class="c17"><a class="c7" href="https://www.google.com/url?q=https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf&amp;sa=D&amp;ust=1599902373873000&amp;usg=AOvVaw2yhM0vOH98j1Bjhsudupta">Sutskever et al., 2014</a></span><span>,</span><span><a class="c7" href="https://www.google.com/url?q=http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf&amp;sa=D&amp;ust=1599902373874000&amp;usg=AOvVaw0rAalaDLvi_HNCHS28Tcnp">&nbsp;</a></span><span class="c17"><a class="c7" href="https://www.google.com/url?q=http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf&amp;sa=D&amp;ust=1599902373874000&amp;usg=AOvVaw0rAalaDLvi_HNCHS28Tcnp">Cho et al., 2014</a></span><span class="c6">).</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6">Understanding these requires understanding a series of concepts that build on top of each other.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><h2 class="c48" id="h.twlxpjwhek5t"><span class="c13">Encoding, decoding and &lsquo;context&rsquo;</span></h2><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6">The &lsquo;model&rsquo; is composed of an encoder and decoder</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>The </span><span class="c65">encoder</span><span>&nbsp;processes the input and compiles a </span><span class="c3">&lsquo;</span><span class="c3 c18">context&rsquo; vector</span><span class="c1">&nbsp;(essentially a list of numbers)</span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4"><span class="c3">&lsquo;context&rsquo; vector </span><span>is then used by </span><span class="c9">decoder</span><span class="c6">&nbsp;to generate output</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6">Example: english sentence ------&gt; context representation ------&gt; french sentence </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 512.00px; height: 384.00px;"><img alt="" src="images/image1.jpg" style="width: 512.00px; height: 2048.00px; margin-left: 0.00px; margin-top: -401.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c1">SIZE OF CONTEXT VECTOR</span></p><p class="c4"><span class="c6">Is determined by the the number of hidden/memory units in the encoder architecture</span></p><p class="c4"><span class="c6">For real world problems it can be set to 256,512 or higher </span></p><p class="c4 c8"><span class="c6"></span></p><h3 class="c33" id="h.74isszup22nm"><span class="c20 c36 c37"></span></h3><h3 class="c49" id="h.66tybkjakar8"><span class="c20 c36 c37">INPUT PROCESSING/WORD EMBEDDING</span></h3><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 615.27px; height: 354.50px;"><img alt="" src="images/image1.jpg" style="width: 615.27px; height: 2461.07px; margin-left: 0.00px; margin-top: -938.52px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c3">Our input needs to be in numeric form</span><span class="c6">, if we are using text as input it needs to be converted to a vector of numbers.</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>To transform a word into a vector, we turn to the class of methods called &ldquo;</span><span class="c17"><a class="c7" href="https://www.google.com/url?q=https://machinelearningmastery.com/what-are-word-embeddings/&amp;sa=D&amp;ust=1599902373879000&amp;usg=AOvVaw21PrHacjRZ_OwVtWBZOtoK">word embedding</a></span><span>&rdquo; </span><span class="c3">algorithms</span><span class="c6">.</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Examples:</span></p><ol class="c23 lst-kix_bncp17qp0hyr-0 start" start="1"><li class="c2 c55"><span class="c6">Countvectorizer</span></li><li class="c43"><span class="c6">TF-IDF Vectorizer</span></li><li class="c43"><span class="c6">Hashing Vectorizer</span></li><li class="c43"><span class="c6">Word2Vec</span></li></ol><p class="c4"><span>Each word is converted to a vector, all embedding vectors have </span><span class="c1">the same length</span></p><p class="c4"><span class="c6">Its common to use embedding vectors of size 200, 300 </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4 c8"><span class="c6"></span></p><h2 class="c48" id="h.i3uneo287idb"><span class="c13">&nbsp;ENCODER DECODER</span></h2><p class="c4"><span>The encoder and decoder tend to both be </span><span class="c62">recurrent neural networks</span><span class="c6">&nbsp;</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c29">But bi-directional </span><span class="c3 c29">lstm</span><span class="c29">, grus (</span><span class="c3 c29">Gated recurrent unit</span><span class="c6 c29">) etc can also be used , these are all types of recurrent architectures. </span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 615.00px; height: 296.00px;"><img alt="" src="images/image1.jpg" style="width: 615.00px; height: 2464.54px; margin-left: 0.00px; margin-top: -1293.85px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c0"><span class="c45 c37 c58">We know the at each stepp rnns accepts two inputs ,</span></p><ol class="c23 lst-kix_cz9ekckz0po4-0 start" start="1"><li class="c0 c55 c69"><span class="c45 c58 c37">Current &nbsp;input from input sequence </span></li><li class="c0 c69 c55"><span class="c45 c58 c37">hidden state input from previous rnn step</span></li></ol><p class="c4 c8"><span class="c6"></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>Since the </span><span class="c3">encoder</span><span>&nbsp;and </span><span class="c3">decoder</span><span class="c6">&nbsp;are both RNNs,</span></p><p class="c4"><span class="c6">each time step one of the RNNs does some processing,</span></p><p class="c4"><span class="c6">And it updates its hidden state based on its inputs and previous inputs it has seen.</span></p><h2 class="c11 c59" id="h.tu2qatqug2ci"><span class="c13"></span></h2><h2 class="c11" id="h.jdvwjex76h02"><span class="c45 c3 c71">HIDDEN STATE FOR ENCODER AND DECODER</span></h2><h2 class="c11" id="h.ouyap7khkb1n"><span class="c13">Encoder accepts one input from sequence, and generates hidden state and output, one by one. </span></h2><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>When it is done with the input sequence the </span><span class="c3">last hidden state </span><span>that it generates i</span><span class="c40">s the </span><span class="c40 c3">CONTEXT VECTOR</span><span class="c3">&nbsp;</span><span>that is passed to the </span><span class="c1">decoder</span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4"><span class="c6">THEN, in similar way, DECODER also generates outputs hidden states one step at a time sequentially </span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 632.83px; height: 279.50px;"><img alt="" src="images/image12.gif" style="width: 632.83px; height: 279.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4"><span class="c17 c3"><a class="c7" href="https://www.google.com/url?q=https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/&amp;sa=D&amp;ust=1599902373887000&amp;usg=AOvVaw3a7j2CR03F4MQCKwpf33cj">Source</a></span><span class="c1">&nbsp;</span></p><h3 class="c49" id="h.ycrmlywnzjtq"><span class="c16">Context vector and </span><span class="c3 c16 c20">attention</span></h3><p class="c4"><span class="c27">How does the </span><span class="c3 c27">decoder, or decoding phase, </span><span class="c34 c27">use the context vector Ct ?</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c45 c3 c57">Attentional hidden state !</span></p><p class="c4"><span>&nbsp;After the context vector is derived it is concatenate with the target</span><span>&nbsp;hidden state </span><span class="c3">ht</span><span>&nbsp;to generate </span><span class="c27 c34">attentional a hidden state for every time,step </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6">We get our predictions (prob distribution) by softmaxing this attentional hidden state * weights. </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 434.50px; height: 373.98px;"><img alt="" src="images/image8.png" style="width: 434.50px; height: 373.98px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c17 c3"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1508.04025v5&amp;sa=D&amp;ust=1599902373890000&amp;usg=AOvVaw1dcdgZ3fY6KEJETKsEQgZT">arXiv:1508.04025v5</a></span><span class="c1">&nbsp;[cs.CL] </span></p><h2 class="c11" id="h.82xtpgqd325h"><span class="c13">Encoder Decoder ATTENTION</span></h2><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>The context vector turned out to be a bottleneck for these types of models. It made it challenging for the models to deal with </span><span class="c1">long sentences.</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>A solution was proposed in</span><span><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1409.0473&amp;sa=D&amp;ust=1599902373891000&amp;usg=AOvVaw2kHnCIbu4BBFpSVgqrhsWO">&nbsp;</a></span><span class="c17"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1409.0473&amp;sa=D&amp;ust=1599902373892000&amp;usg=AOvVaw0rvieV2VqSd8rBDRxfLamu">Bahdanau et al., 2014</a></span><span>&nbsp;and</span><span><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1508.04025&amp;sa=D&amp;ust=1599902373892000&amp;usg=AOvVaw3XqnpktXRmT92aHv_EYS1B">&nbsp;</a></span><span class="c17"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1508.04025&amp;sa=D&amp;ust=1599902373893000&amp;usg=AOvVaw3PpeVRAr1Zt7wox2PZFOIn">Luong et al., 2015</a></span><span>. These papers introduced and refined a technique called </span><span class="c1">&ldquo;Attention&rdquo;</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>Attention allows the model to FOCUS on the i</span><span class="c3">mportant parts of the input</span><span class="c6">, so that it can understand longer sequences with efficiency. </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>This ability to </span><span class="c3">amplify</span><span class="c6">&nbsp;the signal from the relevant part of the input sequence makes attention models produce better results</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c0"><span class="c3">ENCODER WITH </span><span class="c1 c70">GLOBAL ATTENTION</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>Unlike, our previous encoder ATTENTION ENCODER passes </span><span class="c3">ALL OF The hidden states </span><span>to the DECODER instead of the </span><span class="c1">last hidden state</span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4"><span class="c27">Encoder starts accepting our input vector one element at a time, as it processes it it generates hidden states for that time state, it keeps generating hidden states, for each input of the sequence, in the end it passes</span><span class="c3 c27">&nbsp;all hidden state vectors</span><span class="c34 c27">&nbsp;to the decoder instead of the just last one.</span></p><p class="c4 c8"><span class="c3 c27 c46"></span></p><p class="c4 c8"><span class="c1"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 451.25px; height: 393.50px;"><img alt="" src="images/image1.jpg" style="width: 451.25px; height: 1809.32px; margin-left: 0.00px; margin-top: -1168.64px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c8"><span class="c34 c27"></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4 c8"><span class="c28"></span></p><p class="c4 c8"><span class="c28"></span></p><p class="c4"><span class="c28">Calculating GLOBAL CONTEXT VECTOR :</span></p><p class="c4 c8"><span class="c6"></span></p><ol class="c23 lst-kix_fkfklindn7va-0 start" start="1"><li class="c43"><span class="c6">Look at each encoder hidden states it received and associated with word in the input sentence</span></li><li class="c43"><span class="c6">Score each hidden state vector </span></li><li class="c43"><span>Multiply each hidden states by </span><span class="c64">score</span><span>, thus amplifying hidden states with high scores, and </span><span class="c1">drowning out hidden states with low scores</span></li><li class="c43"><span class="c3">Generate a sum using all the scored/weighted </span><span class="c1 c19">VECTORS</span></li></ol><p class="c4 c8"><span class="c1"></span></p><p class="c0"><span class="c3 c10">General idea, The specific way in which all the source hidden states are combined to form the GLOBAL CONTEXT vector varies between model architectures</span><span class="c46 c3 c27">&nbsp;</span></p><p class="c0 c8"><span class="c46 c3 c27"></span></p><p class="c0 c8"><span class="c46 c3 c27"></span></p><p class="c4"><span class="c36">STEPS IN A DECODER </span></p><p class="c4"><span class="c1">At every time step : </span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4"><span>1.The attention decoder RNN takes in end element of the input embedding, , and an </span><span class="c30">initial decoder hidden state</span><span class="c6">. </span></p><p class="c4"><span>2.the RNN processes its inputs, producing an output and a new hidden state vector </span><span class="c30">(h4)</span><span class="c6">. The output is discarded.</span></p><p class="c4"><span>3.Attention Step: We use the &nbsp;</span><span class="c9">encoder hidden states</span><span>&nbsp;and the </span><span class="c30">h4</span><span>&nbsp;vector to calculate a context vector (</span><span class="c24">C4</span><span class="c6">) for this time step.</span></p><p class="c4"><span>4.We concatenate </span><span class="c3 c30">h4</span><span>&nbsp;and </span><span class="c24 c3">C4</span><span>&nbsp;into one vector. WE HAVE COMBINED CURRENT </span><span class="c24">ATTENTION CONTEXT VECTOR</span><span>&nbsp;AND CURRENT </span><span class="c6 c30">DECORED HIDDEN STATE</span></p><p class="c4"><span>5. This combined vector is passed to a FFNN and the </span><span class="c3 c41">output</span><span class="c6">&nbsp;of this network is the output for the current time step. Example next output word</span></p><p class="c4 c8"><span class="c6"></span></p><h3 class="c49" id="h.eyagztmk1lk9"><span class="c20 c3 c68">global VS local attention </span></h3><p class="c4 c8"><span class="c28"></span></p><p class="c4"><span class="c36">Instead of using </span><span class="c3 c36">all the hidden states </span><span class="c36">from the encoding phase, it is possible to create a model that uses a subset of the source hidden states, this can drastically reduce the </span><span class="c3 c36">computation cost </span><span class="c36">and allow the model to </span><span class="c3 c36">work with larger sequences </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6">The way in which selection of the hidden states from the encoder phase is optimized while training will differ from model to model.</span></p><p class="c4 c8"><span class="c6 c22"></span></p><p class="c4 c8"><span class="c6 c22"></span></p><p class="c4"><span class="c22">&nbsp;: attention models passes </span><span class="c3 c22">all/many of the the hidden state vectors</span><span class="c22">&nbsp;generated by the encoder to the decoder instead of just the </span><span class="c22 c47">last &lsquo;context&rsquo; vector</span><span class="c22">, this allows the decoder to process the hidden states and &nbsp;</span><span class="c1 c22">&lsquo;focus&rsquo; on the important elements of the encoded information to generate a better output</span></p><p class="c4 c8"><span class="c1 c22"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 349.57px; height: 428.50px;"><img alt="" src="images/image15.png" style="width: 349.57px; height: 428.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c50 c67" id="h.p6lqdedkj65r"><span class="c3 c35"></span></h1><h1 class="c50" id="h.traral6255b7"><span class="c35 c3">TRANSFORMER!!!</span></h1><p class="c4"><span class="c6">TRANSFORMER is an improvement on our previous attention model.</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>The Transformer was proposed in the paper</span><span><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1706.03762&amp;sa=D&amp;ust=1599902373904000&amp;usg=AOvVaw0OtG-6Z42kzKhrNSKwtXca">&nbsp;</a></span><span class="c17"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1706.03762&amp;sa=D&amp;ust=1599902373905000&amp;usg=AOvVaw0K4FYxNfWKhZskiic5rUoP">Attention is All You Need</a></span><span class="c6">.</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6">HIGH LEVEL : this model similar to our previous attention model uses a encoder-decoder pair</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6">HOWEVER : instead of using linear, rnn, lstm or gru chain. &nbsp;transformer uses a STACK of encoders and decoders [not similar to previous], where and decoding blocks consists of six encoders and six decoders respectively </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c1">ALL ENCODERS have identical architecture, but each one has its own set of weights/learnable parameters that are individually optimized. Same goes for decoders as well. </span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 615.00px; height: 347.00px;"><img alt="" src="images/image1.jpg" style="width: 615.00px; height: 2464.54px; margin-left: 0.00px; margin-top: -2100.85px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h2 class="c11" id="h.jw6gi1b480wm"><span class="c45 c3 c71">WHY TRANSFORMER ?? </span></h2><p class="c4"><span>The core requirement of the </span><span class="c3">transformer</span><span>&nbsp; </span><span class="c17"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1706.03762&amp;sa=D&amp;ust=1599902373907000&amp;usg=AOvVaw2uTGcaHD7wkRkU4XmB6hlA">arXiv:1706.03762</a></span><span>&nbsp;[cs.CL], is to have a model capable of working with sequential data </span><span class="c60">WITHOUT using </span><span class="c3 c60">recurrent models</span><span class="c3">&nbsp;</span><span>&nbsp;such as rnn, lstm</span><span class="c3">&nbsp;</span><span class="c6">&nbsp;or grus.</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>`</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 144.00px;"><img alt="" src="images/image6.png" style="width: 624.00px; height: 144.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c17"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1706.03762&amp;sa=D&amp;ust=1599902373909000&amp;usg=AOvVaw34rClGu-jdSAHSXdTasm03">arXiv:1706.03762</a></span><span class="c6">&nbsp;[cs.CL]</span></p><p class="c4"><span class="c6">The basic sequential nature of these models creates great computation cost and makes working with really long sequences of data impractical. </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>To move beyond the </span><span class="c14">limits of sequential computation </span><span>in this case, we need a model that can work with sequential data but also allows </span><span class="c14">for parallel processing of information</span><span class="c6">, for this reason the transformer was created. </span></p><h2 class="c11" id="h.jkicqwu85mow"><span class="c13">TRANSFORMER ENCODER</span></h2><p class="c4"><span class="c6">Self attention</span></p><p class="c4"><span>The input first flows into the </span><span class="c3">SELF ATTENTION</span><span>&nbsp;layer this allows the encoder to </span><span class="c3">encode the current inpu</span><span>t, WHILE looking considering its relationship with the </span><span class="c3">rest</span><span class="c6">&nbsp;of the input sequence. </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 548.62px; height: 281.50px;"><img alt="" src="images/image3.jpg" style="width: 548.62px; height: 2205.38px; margin-left: 0.00px; margin-top: -34.52px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c8"><span class="c6"></span></p><h2 class="c11" id="h.owcu2pt3wd93"><span class="c13">TRANSFORMER DECODER</span></h2><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>The has both the elements of the encoder, ie. </span><span class="c1">self attention and the FFNN</span></p><p class="c4"><span class="c6">&nbsp;But the decoder uses a special attention layer that helps it further focus on the encoder output, by performing (multi-headed) attention (explained later) over the output of the encoder stack,</span></p><p class="c4"><span class="c6">&nbsp; </span></p><p class="c4"><span class="c6">This is called encoder-decoder attention, and is similar to our first attention model. </span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 615.00px; height: 380.00px;"><img alt="" src="images/image3.jpg" style="width: 615.00px; height: 2460.00px; margin-left: 0.00px; margin-top: -351.50px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4 c8"><span class="c1"></span></p><h2 class="c11" id="h.pa0h22yyk42"><span class="c13">SELF ATTENTION : </span></h2><p class="c4"><span class="c6">Self attention is &nbsp;used both in the encoding as well as the decoding phase, why do we need to use self attention ? </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>Self attention allows us to to find the relationships between different positions of a single sequence, By finding these relationships we can compute a representation of the sequence, </span><span class="c29">that can be decoded to produce output,</span><span class="c6">&nbsp;similar to context vector,</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>Self attention </span><span class="c31">does not rely on the sequential processing</span><span class="c6">&nbsp;of the inputs to compute these relationships. </span></p><p class="c4 c8"><span class="c6"></span></p><h2 class="c11" id="h.f53fjg5i870u"><span class="c3">TRANSFORMER PIPELINE</span><span class="c13"><br></span></h2><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6">Lets look at how the data/tensors flow across our model and its various components at a high level. </span></p><h2 class="c48" id="h.bln5197agxpg"><span class="c13">&nbsp;WORD EMBEDDINGS </span></h2><p class="c4"><span>&nbsp;WORD EMBEDDINGS : with any nlp problem, each word in a sentence is converted into a numeric form, ie an array of a specific size, this array is the numeric data that represents that word, Hypothetical example :</span><span class="c1">&nbsp;input sentence = &lsquo;my name is deepraj&rsquo;</span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4"><span class="c1">&lsquo;My&rsquo; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: [0.3321,0.54651,0.231561&hellip;..]</span></p><p class="c4"><span class="c1">&lsquo;Name&rsquo; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: [0.31321,0.54,0.231541261&hellip;]</span></p><p class="c4"><span class="c1">.&lsquo;is&rsquo; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: [0.3345645621,0.54651,0.23..]</span></p><p class="c4"><span class="c1">.&lsquo;deepraj&rsquo;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: [0.3321,0.54651,0.231561.&hellip;.]</span></p><p class="c4"><span class="c1 c32">Input tensor:</span></p><p class="c4 c8"><span class="c1 c32"></span></p><p class="c4"><span class="c1 c32">[ &nbsp;[0.3321,0.54651,0.231561&hellip;...],</span></p><p class="c4"><span class="c1 c32">&nbsp; &nbsp;[0.31321,0.54,0.231541261&hellip;.],</span></p><p class="c4"><span class="c1 c32">&nbsp; &nbsp;[0.3345645621,0.54651,0.23..],</span></p><p class="c4"><span class="c1 c32">&nbsp; &nbsp;[0.3321,0.54651,0.231561.&hellip;] &nbsp; ]</span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4"><span class="c3">Each word is embedded into an array of the </span><span class="c3 c39">same size</span></p><p class="c4"><span class="c39 c3">There are several techniques to accomplish this task.</span></p><p class="c4"><span class="c40">Only the</span><span class="c40 c3">&nbsp;bottom most encoder</span><span class="c40">&nbsp;receives this </span><span class="c40 c3">embedded data, </span><span class="c6 c40">subsequent encoders receives the output from the previous encoder. </span></p><p class="c4 c8"><span class="c6 c40"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 517.50px; height: 336.05px;"><img alt="" src="images/image3.jpg" style="width: 517.50px; height: 2066.73px; margin-left: 0.00px; margin-top: -610.36px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span>We can see that each input of the sequence flows </span><span class="c3">parallely into the encoder</span><span>, within the encoder there are dependencies between each input, but the feed forward neural network doesn&#39;t have that, </span><span class="c6 c14">as each word takes its own path here, the various paths can be executed in parallel while flowing through the feed-forward layer.</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>We can start to see that this transformer is</span><span class="c6 c29">&nbsp;lending itself to parallel computation </span></p><p class="c4 c8"><span class="c1"></span></p><p class="c0"><span class="c1">ENCODER IN DETAIL </span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 571.00px; height: 703.00px;"><img alt="" src="images/image3.jpg" style="width: 571.00px; height: 2281.65px; margin-left: 0.00px; margin-top: -1086.83px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4"><span class="c6">an encoder receives a list of vectors as input. It processes this list by passing these vectors into a &lsquo;self-attention&rsquo; layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>In the feedforward stage each &lsquo;word&rsquo; passes through a feed forward network ,</span><span class="c29">&nbsp;theyre all passed </span><span class="c3 c29">separately/parallely</span><span class="c29">&nbsp; into the</span><span class="c1 c29">&nbsp;exact same network</span></p><h2 class="c48" id="h.dza1nvt9xu2h"><span class="c13">SELF-ATTENTION</span></h2><p class="c4"><span class="c6">Consider the following sentence :</span></p><p class="c0"><span class="c46 c3 c27">&nbsp;&ldquo;the lion ate the zebra because it was too hungry&rdquo;</span></p><p class="c4"><span class="c6 c29">here does &lsquo;it&rsquo; referring to the lion or to the zebra?</span></p><p class="c4"><span class="c6">For humans this is an easy question because it would be a logical statement only if the hungry LION eats the zebra. And not that the lion at the zebra because the zebra was hungry.</span></p><p class="c4"><span class="c6 c29">For computers its not natural to have an understanding of language like this</span></p><p class="c4 c8"><span class="c6 c29"></span></p><p class="c0"><span class="c6">ROLE OF SELF -ATTENTION:</span></p><p class="c4"><span>This is what self attention helps us with. It will allow the model to understand that in the </span><span class="c6 c29">above sentence &lsquo;it&rsquo; refers to the lion</span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4"><span class="c6">As each &lsquo;word&rsquo; is processed. Self attention layer allows the model to associate with other &lsquo;words&rsquo; in the sequence leading to better encoding. </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6 c18">Self-attention is the method the Transformer uses to associate the knowledge of other relevant words into the one we&rsquo;re currently processing.</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>Simply put if the model is currently processing the word </span><span class="c3">&lsquo;IT&rsquo; </span><span>self attention allows us to see how </span><span class="c3">HEAVILY</span><span class="c6">&nbsp;its related to the rest of the words</span></p><p class="c4"><span class="c6">Simplified Hypothetical illustrative example :</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>It : </span><span class="c3 c27">&nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c3 c27 c52">&nbsp;</span><span class="c27 c52">t</span><span class="c15">he &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:0.11</span></p><p class="c2"><span class="c45 c3 c52">&nbsp;Lion&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.99</span></p><p class="c2"><span class="c15">&nbsp;ate &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:0.2</span></p><p class="c2"><span class="c15">the &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:0.1</span></p><p class="c2"><span class="c15">Zebra&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:0.1</span></p><p class="c2"><span class="c15">Because&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:0.1</span></p><p class="c2"><span class="c15">&nbsp;It&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:1</span></p><p class="c2"><span class="c15">&nbsp;Was&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:0.6</span></p><p class="c2"><span class="c15">&nbsp;Too&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:.02</span></p><p class="c2"><span class="c15">hungry&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:0.12</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c3 c16">CALCULATING SELF ATTENTION</span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 439.50px; height: 377.92px;"><img alt="" src="images/image3.jpg" style="width: 439.50px; height: 1756.19px; margin-left: 0.00px; margin-top: -1370.71px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 446.10px; height: 269.50px;"><img alt="" src="images/image5.jpg" style="width: 446.10px; height: 1870.51px; margin-left: 0.00px; margin-top: -9.42px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c6">In every self attention layer </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>Each input vector/&rsquo;word&rsquo; embedding &nbsp;Is used to create three vectors,</span><span class="c44">&nbsp;</span><span class="c3 c44">query</span><span class="c44">, </span><span class="c3 c44">key</span><span class="c44">&nbsp;and </span><span class="c3 c44">value</span><span class="c6 c44">.</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>This is done by multiplying each input vector with t</span><span class="c18">hree separate weight vectors. </span><span class="c3 c18">Wquery ,Wkey, Wvalue, </span><span class="c18">&nbsp;these weight vectors are optimized during the training process.</span><span class="c6">&nbsp;</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c3">In the end we get</span><span class="c3 c53">&nbsp;3 projections</span><span class="c1">&nbsp;for each input, these are : query, value, key. </span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4 c8"><span class="c6 c31"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 326.67px;"><img alt="" src="images/image2.png" style="width: 624.00px; height: 326.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c17"><a class="c7" href="https://www.google.com/url?q=http://jalammar.github.io/illustrated-transformer/&amp;sa=D&amp;ust=1599902373926000&amp;usg=AOvVaw31SyH08d7dMhcpYfdErZnk">source</a></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6">Now we need to score each word in the input sequence against the current input. </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>We calculate scores for current input by taking the dot product of current input</span><span class="c3">&nbsp;QUERY </span><span class="c6">vector</span></p><p class="c4"><span>To the </span><span class="c3">KEY</span><span class="c6">&nbsp;vectors of the rest of sequence. </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6 c18">Score for x1 = [q1.k1,q1.k2&hellip; &nbsp; .. &nbsp;.q1.Kt]</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6">Score involves every input till and including current input</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 446.00px; height: 249.00px;"><img alt="" src="images/image5.jpg" style="width: 446.00px; height: 1873.98px; margin-left: 0.00px; margin-top: -475.44px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4"><span class="c6">Score are then divided by the square root of dimension of the key vector.</span></p><p class="c4"><span>Then tensor is then passed to a</span><span class="c1">&nbsp;SOFTMAX function</span></p><p class="c4"><span class="c6 c21">Sofmax values are &nbsp;multiplied by value vectors, &nbsp;this allows us to focus on some words and drown out irrelevant words,</span></p><p class="c4 c8"><span class="c6 c21"></span></p><p class="c4"><span>The </span><span class="c3">sixth step</span><span class="c6">&nbsp;is to sum up the weighted value vectors &nbsp;these are the z1,z2.. vectors</span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 357.98px; height: 339.50px;"><img alt="" src="images/image9.png" style="width: 357.98px; height: 339.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c8"><span class="c6 c21"></span></p><p class="c4"><span class="c17"><a class="c7" href="https://www.google.com/url?q=http://jalammar.github.io/illustrated-transformer/&amp;sa=D&amp;ust=1599902373929000&amp;usg=AOvVaw0se1dAPwNFbav2n19xTBQA">Source</a></span></p><p class="c4 c8"><span class="c6 c21"></span></p><p class="c4"><span class="c6 c21">This will result in one sum vector for each input which then can be passed on to the next encoder </span></p><p class="c4 c8"><span class="c6 c21"></span></p><p class="c4 c8"><span class="c6 c21"></span></p><p class="c4 c8"><span class="c6 c21"></span></p><p class="c4 c8"><span class="c6 c21"></span></p><p class="c4 c8"><span class="c6 c21"></span></p><p class="c4 c8"><span class="c6 c21"></span></p><p class="c4 c8"><span class="c6 c21"></span></p><p class="c4 c8"><span class="c6 c21"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 238.67px;"><img alt="" src="images/image7.png" style="width: 624.00px; height: 238.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c17 c21"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1706.03762&amp;sa=D&amp;ust=1599902373931000&amp;usg=AOvVaw3dD1D_LdccdcXqebQvwPhi">arXiv:1706.03762</a></span><span class="c6 c21">&nbsp;[cs.CL]</span></p><p class="c4 c8"><span class="c6 c21"></span></p><p class="c4 c8"><span class="c6 c21"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 184.03px; height: 197.50px;"><img alt="" src="images/image10.png" style="width: 184.03px; height: 197.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c17 c21"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1706.03762&amp;sa=D&amp;ust=1599902373932000&amp;usg=AOvVaw386H6sjjWwKQjjwT3525xG">arXiv:1706.03762</a></span><span class="c6 c21">&nbsp;[cs.CL]</span></p><h3 class="c33" id="h.nw9k8j1ektiv"><span class="c20 c36 c37"></span></h3><h3 class="c49" id="h.nw9k8j1ektiv-1"><span class="c20 c36 c37">Self attention matrix calculations in a glance </span></h3><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 401.50px; height: 535.33px;"><img alt="" src="images/image13.jpg" style="width: 401.50px; height: 535.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c8"><span class="c6 c21"></span></p><p class="c4 c8"><span class="c6 c21"></span></p><h2 class="c26" id="h.2pgpxkkd55je"><span class="c45 c3 c66">MULTI HEADED ATTENTION</span></h2><p class="c4 c8"><span class="c6"></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6">Is when we use multiple sets of Query/Key/Value weight matrices for each self attention layer</span></p><p class="c4"><span>Therefor the self attention layer has multiple &lsquo;heads&rsquo;. </span><span class="c6 c29">Each unit processing a set of q k and v matrices is called one &lsquo;head&rsquo;</span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 384.38px; height: 512.50px;"><img alt="" src="images/image18.jpg" style="width: 384.38px; height: 512.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span>For example if the self attention layer has 8 heads, we will produce </span><span class="c21">8 sets of outputs,</span><span class="c6">&nbsp;as our FEED FORWARD network no expecting 8 sets of inputs we need to condense the self attention output to something the FFNN can accept. </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6 c18">We need to condense the multiple head outputs to one output</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 298.50px; height: 398.23px;"><img alt="" src="images/image17.jpg" style="width: 298.50px; height: 398.23px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6">This is done by CONCATENATE all the outputs from multi head attention and multiply with another WEIGHT matrix </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c0"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 272.68px; height: 282.50px;"><img alt="" src="images/image11.png" style="width: 272.68px; height: 282.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c17 c21"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1706.03762&amp;sa=D&amp;ust=1599902373935000&amp;usg=AOvVaw0BS7mDYDDbCawas7fSkdWt">arXiv:1706.03762</a></span><span class="c21">&nbsp;[cs.CL]</span></p><p class="c4"><span class="c6">see how v k and q have multiple weight matrices and after concatenating all the attention outputs its condensed by one weight matrix </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 483.50px; height: 270.42px;"><img alt="" src="images/image4.png" style="width: 483.50px; height: 270.42px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c17"><a class="c7" href="https://www.google.com/url?q=http://jalammar.github.io/illustrated-transformer/&amp;sa=D&amp;ust=1599902373935000&amp;usg=AOvVaw2oDrPMC03j9LjZBH9rfHHG">Source</a></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c6">THIS condensed tensor is passed to the FEED FORWARD network right after the </span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 150.67px;"><img alt="" src="images/image14.png" style="width: 624.00px; height: 150.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c17 c61"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1706.03762&amp;sa=D&amp;ust=1599902373936000&amp;usg=AOvVaw0ltlinis0j1qwmnOrIkPq8">arXiv:1706.03762</a></span><span class="c6 c61">&nbsp;[cs.CL]</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c1">POSITIONAL ENCODING</span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4"><span class="c6">Positional vector is added to input embeddings so that the word can learn the positional relationship and distance between words. The vector pattern is learnt by the model to understand the positions in the sequence</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c1">RESIDUAL CONNECTION</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>each sub-layer (self-attention, ffnn) in each encoder and decoder has a residual connection around it, and is followed by a</span><span><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1607.06450&amp;sa=D&amp;ust=1599902373937000&amp;usg=AOvVaw325RNKI6exeFoBQQbpgm3g">&nbsp;</a></span><span class="c17"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1607.06450&amp;sa=D&amp;ust=1599902373938000&amp;usg=AOvVaw1bkHYYC3SYz0rZZe8ANx_K">layer-normalization</a></span><span class="c6">&nbsp;step. &nbsp;This allows us to skip any layer that isnt contributing to learning. </span></p><p class="c4 c8"><span class="c6"></span></p><h1 class="c50" id="h.mxlpfkwfzd4"><span class="c35 c37">DECODER</span></h1><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 342.67px;"><img alt="" src="images/image16.gif" style="width: 624.00px; height: 342.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c17"><a class="c7" href="https://www.google.com/url?q=http://jalammar.github.io/illustrated-transformer/&amp;sa=D&amp;ust=1599902373938000&amp;usg=AOvVaw01_Ausvdrxm9XVVtQSVlRU">Source</a></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>We first encode the input sequence to generate attention </span><span class="c3">vectors K and V</span><span class="c6">&nbsp;</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c3">Decoder self attention - future positions of the sequence are masked from the decoders by setting them to -inf before sending to softmax function</span><span class="c6">&nbsp; &nbsp;</span></p><p class="c0"><span class="c12">&lsquo;</span><span class="c27 c42">We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to&minus;&infin;) all values in the input of the softmax which correspond to illegal connections. </span><span class="c12">&rsquo;</span></p><p class="c0"><span class="c17 c27"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1706.03762&amp;sa=D&amp;ust=1599902373939000&amp;usg=AOvVaw3qrfg9sN-G3vtYuBc2vV-0">arXiv:1706.03762</a></span><span class="c27">&nbsp;[cs.CL]</span></p><p class="c4"><span class="c6">The decoder stack outputs a vector of floats this is fed to a fully connected network that projects a large vector called logits vector. </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c3">The softmax score of this vector decides the output &lsquo;word&rsquo;,</span><span class="c6">&nbsp;this vector will be as large as the number of unique words our model knows. </span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 446.00px; height: 419.00px;"><img alt="" src="images/image5.jpg" style="width: 446.00px; height: 1873.98px; margin-left: 0.00px; margin-top: -1241.44px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c27">&ldquo; i</span><span class="c27">n &quot;encoder-decoder attention&quot; layers, the </span><span class="c27 c54">queries come from the previous decoder layer,</span><span class="c27">and the memory keys and values come from the output of the encoder. </span><span class="c5">This allows every position in the decoder to attend over all positions in the input sequence</span><span class="c27">. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models &ldquo;</span></p><p class="c0"><span class="c17 c27"><a class="c7" href="https://www.google.com/url?q=https://arxiv.org/abs/1706.03762&amp;sa=D&amp;ust=1599902373941000&amp;usg=AOvVaw3RCFYX095N_G-HxOKINRhQ">arXiv:1706.03762</a></span><span class="c27">&nbsp;[cs.CL]</span></p><p class="c4"><span class="c1">TRAINING</span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4"><span class="c1">LOSS FUNCTION</span></p><p class="c4 c8"><span class="c1"></span></p><p class="c4"><span class="c6">We compare output and target probability distributions to generate our loss </span></p><p class="c4"><span class="c6">In this case we are dealing with sets of probability distributions, </span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span class="c1">beam search</span></p><p class="c4 c8"><span class="c6"></span></p><p class="c4"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Instead of comparing only the output with the highest probability, we can try the top two , three etc, to generate the least loss, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;This means we keep two/ three etc partial </span><span class="c3">hypothesis</span><span class="c6">&nbsp;in memory as while training, </span></p></body></html>